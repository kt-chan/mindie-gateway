user nginx;
worker_processes auto;
error_log /var/log/nginx/error.log notice;
pid /var/run/nginx/nginx.pid;

events {
    worker_connections 1024;
}

http {
    include /etc/nginx/mime.types;
    default_type application/octet-stream;

    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for"';

    access_log /var/log/nginx/access.log main;

    sendfile on;
    keepalive_timeout 65;

    # Mock vLLM server on port 8000
    server {
        listen 8000;
        server_name localhost;

        # Mock chat completions endpoint
        location /v1/chat/completions {
            add_header Content-Type application/json;
            return 200 '{"model": "gpt-4o-mock", "choices": [{"message": {"role": "assistant", "content": "Hello! This is a mock response from the vLLM API gateway. Your API key was validated successfully."}}]}';
        }

        # Mock models endpoint
        location /v1/models {
            add_header Content-Type application/json;
            return 200 '{"object": "list", "data": [{"id": "gpt-4o-mock", "object": "model"}]}';
        }

        # Health check endpoint
        location /health {
            add_header Content-Type application/json;
            return 200 '{"status": "ok"}';
        }

        # Default response for other paths
        location / {
            add_header Content-Type application/json;
            return 404 '{"error": "Not Found", "message": "The requested resource was not found"}';
        }
    }

    # API Gateway server on port 80
    server {
        listen 80;
        server_name localhost;

        location /v1/chat/completions {
            # Always validate API key first
            auth_request /auth;
            error_page 401 = @fallback_auth;

            # Otherwise, proxy to vLLM server
            proxy_pass http://127.0.0.1:8000/v1/chat/completions;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            proxy_set_header Authorization $http_authorization;

            # Timeout settings
            proxy_connect_timeout 60s;
            proxy_send_timeout 60s;
            proxy_read_timeout 60s;
        }

        location /auth {
            internal;
            proxy_pass http://127.0.0.1:8020/validate;
            proxy_set_header Authorization $http_authorization;
            proxy_pass_request_body off;
            proxy_set_header Content-Length "";
        }

        location @fallback_auth {
            return 401 "Unauthorized: Invalid API key";
        }

        location /health {
            proxy_pass http://127.0.0.1:8020/health;
        }

        # vLLM server health check
        location /v1/models {
            proxy_pass http://127.0.0.1:8000/v1/models;
            proxy_set_header Host $host;
        }

        # Simple status page
        location / {
            add_header Content-Type text/plain;
            return 200 "vLLM API Gateway is running. Use /v1/chat/completions with your API key.";
        }

        # Default error page
        error_page 500 502 503 504 /50x.html;
        location = /50x.html {
            root /usr/share/nginx/html;
        }
    }
}

